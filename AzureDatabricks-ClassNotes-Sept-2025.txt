
  Databricks
  ----------------------------
   Databricks Basics
	- Databricks basic features
	- Notebook basics & Magic commands
	- Databricks Utilities - fs, widgets, notebook 
	- Data Visualization  
   PySpark Essentials
	- Spark basics
	- Spark SQL basics
	- Spark Structured Streaming basic
   Databricks Lakehouse - Delta Lake
	â€“ Delta Lake using SQL
	- Delta Lake using Python
	- Delta Lake Features
   ELT with Spark SQL
	- Querying from files
	- Writing to tables
	- Schema Validation & Management
	- HOFs & UDFs
   Incremental Data Processing 
	- COPY INTO command
	- Structured Streaming
	- AutoLoader
	- Medallion Pattern
   Production pipelines	
	- Delta Live Tables
	- Job Workflow
   Data Governance basics
	- Unity Catalog


  Materials
  ---------
   - PDF presentations
   - Code Modules (PySpark)
   - Databricks Notebooks (DBC files)
   - Class notes (PySpark)
   - Github: 
	https://github.com/ykanakaraju/pyspark
	https://github.com/ykanakaraju/databrickscts


  Spark
  -----

    => Spark is in-memory distributed computing framework for big data analytics.

	in-memory: ability to persist intermediate results in RAM and subsequent operations
		   can directly work on these persisted intermediate results. 

    => Spark is written in Scala programming language

    => Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

    => Spark applications can run on:
	-> local, Spark standalone, YARN, Mesos, Kubernetes

    => Spark is unified analytics framework.
	

  Spark Unified Framework
  -----------------------

    Spark provides a consistent set of APIs for performing different analytics workloads
    using the same execution engine and some well defined data abstractions and operations.

	Batch Analytics			-> Spark SQL
	Streaming Analytics (real time)	-> Structured Streaming, DStreams API
	Predictive analytics (ML)	-> Spark MLLib  (mllib & ml)
	Graph parallel computations  	-> Spark GraphX


  Getting started with Spark
  -------------------------- 

   1. Setting up local dev environment

	Ref URL: https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf  
	Spark download: https://spark.apache.org/downloads.html


   2. Databricks Free Edition (preferred)


  Setting up PySpark on Windows machine
  -------------------------------------

	1. Install Java 8 or up

		java -version
		
		https://www.oracle.com/in/java/technologies/downloads/#jdk24-windows

	2. Add JAVA_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add JAVA_HOME env. variable with the PATH were Java is installed.

	3. Download and extract Spark binaries.

		URL: https://spark.apache.org/downloads.html
		Choose the version
		Download spark: <click on this link>
		Go to the mirror site and download


		-> Extract the downloaded file in a suitable folder
		(tar -xvf <path-of-tgz file>)

	4. Add SPARK_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add SPARK_HOME env. variable with the PATH were Spark is extracted.

	5. Setup Hadoop winutils for windows 

		URL: https://github.com/cdarlint/winutils
		Download the repo
		Extract it and copy the folder to your Spark path. 

	6. Add HADOOP_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add HADOOP_HOME env. variable with the PATH were hadoop is extracted.

	7. Add the "bin" folders of the above to the PATH environment variable

		%JAVA_HOME%\bin
		%SPARK_HOME%\bin
		%HADOOP_HOME%\bin

	8. Open a command terminal and type spark-shell

		C:> spark-shell

	9. Download and install Python (3 or above)

	10. Pip install find-spark library for python

		pip install findspark


  Getting started with Spark on Databricks Free Edition
  -----------------------------------------------------

   -> Click on https://www.databricks.com/try-databricks
   -> Click on "Try Databricks" button (top-right corner)
   -> Click on "Click here" link towards the bottom
	** Looking for Databricks Free Edition? Click here
   -> Provide a valid email address 
   -> Enter the validation code sent to the email to login to Databricks free edition

	-> Login link: http://login.databricks.com



   Databricks 'dbutils'
   --------------------

	'help' command
	------------
	dbutils.help()
	dbutils.fs.help()
	dbutils.fs.help('ls')


	'ls' command
	------------
	dbutils.fs.ls("/")
	dbutils.fs.ls("/FileStore")
		list paths => [ d[0] for d in dbutils.fs.ls("/FileStore")]
	dbutils.fs.ls("/FileStore/testdata/csv")


	'mkdirs' command
	----------------
	dbutils.fs.mkdirs("/FileStore/testdata2")
	dbutils.fs.ls("/FileStore")


	'cp' command to copy files between DBFS directories
	---------------------------------------------------
	dbutils.fs.cp("/FileStore/testdata/csv/2011_summary.csv", "/FileStore/testdata2")
	dbutils.fs.cp("/FileStore/testdata/csv/2012_summary.csv", "/FileStore/testdata2")
	dbutils.fs.cp("/FileStore/testdata/csv/2013_summary.csv", "/FileStore/testdata2")

	dbutils.fs.ls("/FileStore/testdata/csv")


	'mv' command for moving or renaming files
	------------------------------------------
	dbutils.fs.help('mv')

	dbutils.fs.mv("/FileStore/testdata/csv", "/FileStore/testdata3", recurse=True)
	dbutils.fs.ls('/FileStore/testdata3')

	dbutils.fs.mv('/FileStore/testdata3/2011_summary.csv', '/FileStore/testdata3/2011_summary_renamed.csv')


	'rm' command to remove files & directories from DBFS
	-----------------------------------------------------
	dbutils.fs.rm('/FileStore/testdata3/2011_summary_renamed.csv')
	dbutils.fs.rm('/FileStore/testdata3/', recurse=True)




  Spark Architecture
  ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 


	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> Receives the tasks from the Driver
		-> All tasks run the same code but on different partitions of the data
		-> The status of tasks are reported to the driver. 


  Spark DAG Scheduler
  -------------------

	Application  (SparkContext represents an application)
	|
	|--> Jobs  (each action command launches a Job)
		|
		|--> Stages (each wide transformation in the RDD DAG of a job causes stage transition)
			|
			|--> Tasks (each task has multiple transformations that run in parallel)




  ================================
     Spark SQL (pyspark.sql)
  ================================ 

    -> High Level API built on top of Spark Core

	File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format  : RDBMS, NoSQL
	Hive Format  : Hive Warehouse

   SparkSession
   ------------

	-> Starting point of execution
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()   


   DataFrame (DF)
   --------------
	-> Main data abstraction of Spark SQL
	-> Is a collection of distributed in-memory partitions
	-> Immutable
	-> Lazily evaluated

	-> DataFrame is a collection of "Row" objects.

	-> DataFrame contains two components:
		-> Data    : Collection of 'Row' objects
		-> Schema  : StructType object

			StructType(
			   [
				StructField('age', LongType(), True), 
				StructField('gender', StringType(), True), 
				StructField('name', StringType(), True), 
				StructField('phone', StringType(), True), 
				StructField('userid', LongType(), True)
			   ]
			)
 	
	

   Basic steps in creating a Spark SQL Application
   -----------------------------------------------

	1. Read/load data from some data-source into a DataFrame 		

		#df1 = spark.read.format("json").load(inputData)
		#df1 = spark.read.load(inputData, format="json")
		df1 = spark.read.json(inputData)


	2. Transform the DF using DF transformation methods or using SQL

	        Using DF transformation methods
		-------------------------------	
		 df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


		Using SQL
		---------		
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()

		NOTE: You can drop temp-view:  
		      -> spark.catalog.dropTempView("users")


	3. Save the dataframe into some external destination (such as files/databases/hive etc)

		df2.write.format("json").save("/FileStore/output/json")
		df2.write.save("/FileStore/output/json", format="json")
		df2.write.json("/FileStore/output/json")









	











